# Secure Data Processing Platform - Complete System Build Specification

## System Overview

Build an AI-powered reseller data management platform that transforms messy Excel spreadsheets into clean, actionable insights through automatic data processing, real-time dashboards, and AI-powered natural language analytics.

**SECURITY FIRST**: This system is designed for complete data isolation. Each client deployment must use entirely separate infrastructure with no shared resources.

## Core Architecture

### Technology Stack
- **Backend**: FastAPI (Python 3.12+)
- **Frontend**: React 18+ with Vite
- **Database**: PostgreSQL with Row-Level Security (client-managed)
- **AI Integration**: OpenAI GPT-4 with LangChain (client API keys)
- **Charts**: Chart.js with react-chartjs-2
- **Authentication**: JWT (client-generated secrets)
- **File Processing**: pandas + openpyxl
- **Background Tasks**: Celery with Redis (optional, client-managed)

### Security Principles
1. **Complete Data Isolation**: No shared databases, keys, or infrastructure
2. **Client-Owned Infrastructure**: Each deployment uses client's own cloud resources
3. **Zero Trust Architecture**: No pre-configured connections or defaults
4. **Client-Generated Secrets**: All encryption keys generated by client
5. **No Cross-Client Dependencies**: System cannot access external databases

### System Components
1. **File Upload & Processing Pipeline**
2. **Vendor Detection & Data Normalization**
3. **Real-time Dashboard with Analytics**
4. **AI-Powered Chat Interface**
5. **Email Reporting System**
6. **User Authentication & Security**

## Database Schema Playbook

### Database Setup Requirements

**CRITICAL**: Each client must set up their own isolated PostgreSQL database with the following characteristics:
- Dedicated database instance (no shared hosting)
- Client-generated database credentials
- Client-controlled access policies
- No external database connections allowed

### Core Tables Structure

```sql
-- Enable UUID extension
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- Users table (for client's authentication system)
CREATE TABLE users (
  id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
  email text UNIQUE NOT NULL,
  password_hash text NOT NULL, -- Client must implement secure hashing
  created_at timestamptz DEFAULT now(),
  updated_at timestamptz DEFAULT now(),
  is_active boolean DEFAULT true
);

-- Products catalog (client-specific products only)
CREATE TABLE products (
  ean text PRIMARY KEY,
  name text,
  brand text, -- Client configures their own brand
  created_at timestamptz DEFAULT now(),
  client_id uuid NOT NULL -- Ensures data belongs to this client only
);

-- Upload tracking
CREATE TABLE public.uploads (
  id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id uuid REFERENCES public.users(id) ON DELETE CASCADE,
  filename text NOT NULL,
  file_size bigint,
  uploaded_at timestamptz DEFAULT now(),
  status text CHECK (status IN ('pending', 'processing', 'completed', 'failed')),
  error_message text,
  rows_processed integer,
  rows_cleaned integer,
  processing_time_ms integer
);

-- Main sales data table (isolated per client)
CREATE TABLE sales_data (
  id uuid NOT NULL DEFAULT gen_random_uuid(),
  upload_id uuid REFERENCES uploads(id) ON DELETE SET NULL,
  product_ean text REFERENCES products(ean),
  month integer CHECK (month >= 1 AND month <= 12),
  year integer CHECK (year >= 2000),
  quantity integer,
  sales_amount_local text, -- Local currency amount
  sales_amount_base numeric, -- Base currency for client
  currency text,
  created_at timestamp DEFAULT now(),
  reseller text,
  functional_name text,
  client_id uuid NOT NULL, -- Mandatory client isolation
  CONSTRAINT sales_data_pkey PRIMARY KEY (id)
);

-- Processed/cleaned data storage (client-isolated)
CREATE TABLE processed_data (
  id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
  upload_id uuid REFERENCES uploads(id) ON DELETE CASCADE,
  product_ean text,
  month integer,
  year integer,
  quantity integer,
  sales_amount_local text,
  sales_amount_base numeric,
  currency text,
  reseller text,
  functional_name text,
  sales_date date,
  created_at timestamptz DEFAULT now(),
  client_id uuid NOT NULL -- Ensures no cross-client data access
);

-- Data transformation audit trail
CREATE TABLE public.transform_logs (
  id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
  upload_id uuid REFERENCES public.uploads(id) ON DELETE CASCADE,
  row_index integer,
  column_name text,
  original_value text,
  cleaned_value text,
  transformation_type text,
  logged_at timestamptz DEFAULT now()
);

-- Step-by-step processing logs
CREATE TABLE public.processing_logs (
  id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
  upload_id uuid REFERENCES public.uploads(id) ON DELETE CASCADE,
  step_name text NOT NULL,
  step_status text CHECK (step_status IN ('started', 'completed', 'failed')),
  message text,
  details jsonb,
  created_at timestamptz DEFAULT now()
);
```

### Row-Level Security (RLS)

```sql
-- Enable RLS on all tables
ALTER TABLE public.users ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.uploads ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.sellout_entries2 ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.mock_data ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.transform_logs ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.processing_logs ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.products ENABLE ROW LEVEL SECURITY;

-- User policies
CREATE POLICY "Users can view own profile" ON public.users
  FOR SELECT USING (auth.uid() = id);

CREATE POLICY "Users can view own uploads" ON public.uploads
  FOR SELECT USING (auth.uid() = user_id);

CREATE POLICY "Users can create own uploads" ON public.uploads
  FOR INSERT WITH CHECK (auth.uid() = user_id);

-- Data access policies (users only see their own data)
CREATE POLICY "Users can view own sellout entries" ON public.sellout_entries2
  FOR SELECT USING (
    EXISTS (
      SELECT 1 FROM public.uploads
      WHERE uploads.id = sellout_entries2.upload_id
      AND uploads.user_id = auth.uid()
    )
  );

-- Service role policies for background processing
CREATE POLICY "Service role can manage all data" ON public.uploads
  FOR ALL USING (auth.role() = 'service_role');

-- Products are readable by all
CREATE POLICY "Anyone can view products" ON public.products
  FOR SELECT USING (true);
```

### SQL Function for Chat Queries

```sql
CREATE OR REPLACE FUNCTION execute_sql_query(query_text text)
RETURNS jsonb
LANGUAGE plpgsql
SECURITY DEFINER
AS $$
DECLARE
  result jsonb;
  rec record;
  results jsonb[] := '{}';
BEGIN
  -- Security: Only allow SELECT statements
  IF NOT (upper(trim(query_text)) LIKE 'SELECT%') THEN
    RAISE EXCEPTION 'Only SELECT statements are allowed';
  END IF;
  
  -- Security: Block dangerous keywords
  IF upper(query_text) ~ '(DROP|DELETE|UPDATE|INSERT|ALTER|CREATE|GRANT|REVOKE|EXEC|EXECUTE)' THEN
    RAISE EXCEPTION 'Dangerous SQL keywords are not allowed';
  END IF;
  
  FOR rec IN EXECUTE safe_query LOOP
    results := results || to_jsonb(rec);
  END LOOP;
  
  -- Return results as JSON
  RETURN array_to_json(results)::jsonb;
  
EXCEPTION
  WHEN OTHERS THEN
    -- Log security violation attempts
    INSERT INTO security_logs (client_id, attempted_query, error_message, created_at)
    VALUES (client_id_param, query_text, SQLERRM, now());
    
    -- Return sanitized error (don't expose internal structure)
    RETURN jsonb_build_object(
      'error', true,
      'message', 'Query execution failed',
      'timestamp', now()
    );
END;
$$;

-- Security audit log table (CLIENT MUST IMPLEMENT)
CREATE TABLE security_logs (
  id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
  client_id uuid NOT NULL,
  attempted_query text,
  error_message text,
  created_at timestamptz DEFAULT now(),
  ip_address inet,
  user_agent text
);
```

## Backend Implementation

### Project Structure
```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ auth.py          # JWT authentication
â”‚   â”‚   â”œâ”€â”€ upload.py        # File upload endpoints
â”‚   â”‚   â”œâ”€â”€ status.py        # Processing status
â”‚   â”‚   â”œâ”€â”€ dashboard.py     # Analytics endpoints
â”‚   â”‚   â”œâ”€â”€ chat.py          # AI chat interface
â”‚   â”‚   â”œâ”€â”€ email.py         # Email reporting
â”‚   â”‚   â””â”€â”€ webhook.py       # External integrations
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ auth.py          # User models
â”‚   â”‚   â””â”€â”€ upload.py        # Upload models
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ cleaners.py      # Data cleaning logic
â”‚   â”‚   â”œâ”€â”€ detector.py      # Vendor detection
â”‚   â”‚   â””â”€â”€ normalizers.py   # Data normalization
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ auth_service.py     # Authentication logic
â”‚   â”‚   â”œâ”€â”€ cleaning_service.py # File processing
â”‚   â”‚   â”œâ”€â”€ db_service.py       # Database operations
â”‚   â”‚   â”œâ”€â”€ email_service.py    # Email functionality
â”‚   â”‚   â”œâ”€â”€ file_service.py     # File handling
â”‚   â”‚   â””â”€â”€ report_service.py   # Report generation
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py        # Configuration
â”‚       â””â”€â”€ exceptions.py    # Custom exceptions
â”œâ”€â”€ main.py                  # FastAPI app entry point
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ run.sh                  # Startup script
```

### Dependencies (requirements.txt)
```
fastapi==0.109.0
uvicorn[standard]==0.27.0
python-multipart==0.0.6
pandas==2.1.4
openpyxl==3.1.2
supabase==2.9.0
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-dotenv==1.0.0
pydantic==2.5.3
pydantic-settings==2.1.0
email-validator==2.1.0
jinja2==3.1.2
reportlab==4.0.7
celery[redis]==5.3.4
requests==2.31.0
langchain==0.2.16
langchain-openai==0.1.25
langchain-community==0.2.17
psycopg2-binary==2.9.9
sqlalchemy==2.0.30
```

### Core API Endpoints

#### Authentication API (`/api/auth`)
```python
@router.post("/register")
async def register(user_data: UserRegistration)

@router.post("/login") 
async def login(credentials: UserLogin)

@router.get("/debug-token")
async def debug_token(current_user = Depends(get_current_user))

@router.post("/logout")
async def logout()
```

#### Upload API (`/api/upload`)
```python
@router.post("/")
async def upload_file(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    current_user: dict = Depends(get_current_user)
)

@router.post("/multiple")
async def upload_multiple_files(
    files: List[UploadFile],
    current_user: dict = Depends(get_current_user)
)
```

#### Status API (`/api/status`)
```python
@router.get("/")
async def get_upload_status(current_user = Depends(get_current_user))

@router.get("/{upload_id}")
async def get_specific_upload_status(upload_id: str)

@router.get("/{upload_id}/logs")
async def get_processing_logs(upload_id: str)
```

#### Dashboard API (`/api/dashboards`)
```python
@router.get("/analytics")
async def get_analytics_data(current_user = Depends(get_current_user))

@router.get("/sales-summary")
async def get_sales_summary()

@router.get("/monthly-trends")
async def get_monthly_trends()

@router.get("/top-products")
async def get_top_products()
```

#### Chat API (`/api/chat`)
```python
@router.post("/query")
async def chat_query(request: ChatRequest)

@router.get("/suggestions")
async def get_query_suggestions()
```

### Vendor Detection System

Implement intelligent vendor detection based on filename patterns and Excel content:

```python
class VendorDetector:
    def detect_vendor(self, filename: str, df: pd.DataFrame) -> str:
        """Detect vendor based on filename and sheet content"""
        
        filename_lower = filename.lower()
        
        # Filename pattern detection
        patterns = {
            "galilu": ["galilu"],
            "boxnox": ["boxnox"],
            "skins_sa": ["skins sa"],
            "skins_nl": ["bibbiparfu"],
            "cdlc": ["cdlc", "bibbi.*sell_out"],
            "liberty": ["continuity"],
            "taskifai": ["demo", "taskif"]  # Add this for the demo
        }
        
        for vendor, pattern_list in patterns.items():
            if any(re.search(pattern, filename_lower) for pattern in pattern_list):
                return vendor
        
        # Sheet name detection
        if hasattr(df, 'sheet_names'):
            sheet_names = [s.lower() for s in df.sheet_names]
            if any("salespersku" in s for s in sheet_names):
                return "skins_nl"
            elif any("sell out by ean" in s for s in sheet_names):
                return "boxnox"
        
        return "unknown"
```

### Data Cleaning Pipeline

```python
class DataCleaner:
    async def clean_data(self, df: pd.DataFrame, vendor: str, filename: str) -> Tuple[pd.DataFrame, List[Dict]]:
        """Clean data based on vendor-specific rules"""
        
        df_clean = df.copy()
        transformations = []
        
        # Remove empty rows
        df_clean = df_clean.dropna(how='all')
        
        # Apply vendor-specific cleaning
        if vendor == "taskifai":
            df_clean = await self._clean_taskifai_data(df_clean, filename)
        elif vendor == "galilu":
            df_clean = await self._clean_galilu_data(df_clean)
        # ... other vendors
        
        # Standardize numeric values
        df_clean = self._standardize_numeric_columns(df_clean)
        
        return df_clean, transformations
    
    def _clean_numeric_value(self, value):
        """Standardized numeric cleaning for sales values"""
        if pd.isna(value) or value == '':
            return None
        
        clean_value = str(value).strip()
        
        # Replace various dash types with ASCII hyphen
        clean_value = re.sub(r'[\u2013\u2014\u2212]', '-', clean_value)
        
        # Replace comma decimal separator with dot
        clean_value = clean_value.replace(',', '.')
        
        # Remove currency symbols
        clean_value = clean_value.replace('$', '').replace('Â£', '').replace('â‚¬', '').replace(' ', '')
        
        try:
            return float(clean_value)
        except (ValueError, TypeError):
            return None
```

### AI Chat Integration

```python
class ChatService:
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4o",
            temperature=0.1,
            max_tokens=1000
        )
        
    async def process_query(self, message: str, user_id: str) -> dict:
        """Process natural language query about user's data"""
        
        # Create custom SQL database interface
        db = SupabaseSQLDatabase(user_id)
        
        # Create SQL agent
        toolkit = SQLDatabaseToolkit(db=db, llm=self.llm)
        agent = create_sql_agent(
            llm=self.llm,
            toolkit=toolkit,
            agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True
        )
        
        try:
            response = agent.run(message)
            return {
                "response": response,
                "sql_query": self._extract_sql_from_response(response),
                "success": True
            }
        except Exception as e:
            return {
                "response": f"I couldn't process that query: {str(e)}",
                "success": False
            }
```

## Frontend Implementation

### Project Structure
```
frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ App.jsx              # Main app component
â”‚   â”œâ”€â”€ main.jsx            # React entry point
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ AnalyticsDashboard.jsx  # Main dashboard
â”‚   â”‚   â”œâ”€â”€ Upload.jsx              # File upload
â”‚   â”‚   â”œâ”€â”€ DataVisualization.jsx   # Charts
â”‚   â”‚   â”œâ”€â”€ Chat.jsx               # AI chat interface
â”‚   â”‚   â”œâ”€â”€ ChatSection.jsx        # Chat component
â”‚   â”‚   â”œâ”€â”€ StatusList.jsx         # Upload status
â”‚   â”‚   â”œâ”€â”€ EmailReporting.jsx     # Email reports
â”‚   â”‚   â””â”€â”€ LandingPage.jsx        # Welcome page
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ Dashboard.jsx     # Dashboard page
â”‚   â”‚   â””â”€â”€ Login.jsx        # Authentication
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ api.js          # API client
â”‚   â”‚   â””â”€â”€ supabase.js     # Supabase client
â”‚   â”œâ”€â”€ styles/
â”‚   â”‚   â””â”€â”€ index.css       # Global styles
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ mathRenderer.jsx  # Math equation rendering
â”œâ”€â”€ package.json
â”œâ”€â”€ vite.config.js
â””â”€â”€ index.html
```

### Dependencies (package.json)
```json
{
  "name": "data-cleaning-frontend",
  "version": "1.0.0",
  "type": "module",
  "dependencies": {
    "@supabase/supabase-js": "^2.39.3",
    "chart.js": "^4.5.0",
    "katex": "^0.16.22",
    "react": "^18.2.0",
    "react-chartjs-2": "^5.3.0",
    "react-dom": "^18.2.0",
    "react-katex": "^3.1.0"
  },
  "devDependencies": {
    "@types/react": "^18.2.47",
    "@types/react-dom": "^18.2.18",
    "@vitejs/plugin-react": "^4.2.1",
    "vite": "^7.0.5"
  },
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview"
  }
}
```

### Main App Component
```jsx
function App() {
  const [isAuthenticated, setIsAuthenticated] = useState(false)
  const [loading, setLoading] = useState(true)
  const [user, setUser] = useState(null)

  useEffect(() => {
    checkAuthStatus()
  }, [])

  const checkAuthStatus = async () => {
    try {
      const token = localStorage.getItem('access_token')
      if (!token) {
        setLoading(false)
        return
      }

      const response = await fetch('http://localhost:8000/api/auth/debug-token', {
        headers: { 'Authorization': `Bearer ${token}` }
      })

      if (!response.ok) throw new Error('Token validation failed')

      const debugInfo = await response.json()
      if (debugInfo.user_found) {
        setIsAuthenticated(true)
        setUser({
          email: debugInfo.user_email,
          id: debugInfo.user_id
        })
      } else {
        localStorage.removeItem('access_token')
        setIsAuthenticated(false)
      }
    } catch (error) {
      localStorage.removeItem('access_token')
      setIsAuthenticated(false)
    } finally {
      setLoading(false)
    }
  }

  if (loading) return <div className="loading">Loading...</div>

  return (
    <div className="app">
      {!isAuthenticated ? (
        <Login onLoginSuccess={checkAuthStatus} />
      ) : (
        <Dashboard user={user} onLogout={() => {
          localStorage.removeItem('access_token')
          setIsAuthenticated(false)
          setUser(null)
        }} />
      )}
    </div>
  )
}
```

### Dashboard Component
```jsx
const Dashboard = ({ user, onLogout }) => {
  const [activeTab, setActiveTab] = useState('upload')
  const [uploads, setUploads] = useState([])
  const [analytics, setAnalytics] = useState(null)

  // Tab navigation
  const tabs = [
    { id: 'upload', name: 'Upload', icon: 'ðŸ“¤' },
    { id: 'analytics', name: 'Analytics', icon: 'ðŸ“Š' },
    { id: 'chat', name: 'AI Chat', icon: 'ðŸ¤–' },
    { id: 'email', name: 'Reports', icon: 'ðŸ“§' }
  ]

  return (
    <div className="dashboard">
      <header className="dashboard-header">
        <h1>TaskifAI Dashboard</h1>
        <div className="user-info">
          <span>{user?.email}</span>
          <button onClick={onLogout}>Logout</button>
        </div>
      </header>
      
      <nav className="dashboard-nav">
        {tabs.map(tab => (
          <button
            key={tab.id}
            className={`nav-tab ${activeTab === tab.id ? 'active' : ''}`}
            onClick={() => setActiveTab(tab.id)}
          >
            {tab.icon} {tab.name}
          </button>
        ))}
      </nav>
      
      <main className="dashboard-content">
        {activeTab === 'upload' && <Upload />}
        {activeTab === 'analytics' && <AnalyticsDashboard />}
        {activeTab === 'chat' && <ChatSection />}
        {activeTab === 'email' && <EmailReporting />}
      </main>
    </div>
  )
}
```

### Data Visualization Component
```jsx
const DataVisualization = ({ results, sqlQuery, originalMessage }) => {
  const [viewMode, setViewMode] = useState('auto')
  const [chartType, setChartType] = useState('bar')
  
  const analyzeDataForVisualization = () => {
    if (!results || results.length === 0) return null
    
    const firstRow = results[0]
    const columns = Object.keys(firstRow)
    const numericColumns = columns.filter(col => {
      return results.some(row => 
        row[col] !== null && 
        !isNaN(Number(row[col])) &&
        Number(row[col]) !== 0
      )
    })
    
    const textColumns = columns.filter(col => 
      !numericColumns.includes(col) && 
      typeof firstRow[col] === 'string'
    )
    
    return {
      columns,
      numericColumns,
      textColumns,
      hasNumericData: numericColumns.length > 0,
      hasCategories: textColumns.length > 0,
      suggestedChart: getSuggestedChartType(results, numericColumns, textColumns)
    }
  }

  const createChartData = (type) => {
    const analysis = analyzeDataForVisualization()
    if (!analysis?.hasNumericData || !analysis?.hasCategories) return null

    const labelColumn = analysis.textColumns[0]
    const valueColumns = analysis.numericColumns.slice(0, 3)
    
    const labels = results.map(row => String(row[labelColumn] || 'Unknown'))

    if (type === 'pie' || type === 'doughnut') {
      const values = results.map(row => Number(row[valueColumns[0]]) || 0)
      return {
        labels,
        datasets: [{
          data: values,
          backgroundColor: [
            '#FF6384', '#36A2EB', '#FFCE56', '#4BC0C0',
            '#9966FF', '#FF9F40', '#FF6384', '#C9CBCF'
          ],
          label: valueColumns[0]
        }]
      }
    }

    // Bar/Line charts with multiple series
    const datasets = valueColumns.map((col, index) => ({
      label: col,
      data: results.map(row => Number(row[col]) || 0),
      backgroundColor: `hsl(${index * 60}, 70%, 50%)`,
      borderColor: `hsl(${index * 60}, 70%, 40%)`,
      borderWidth: 2,
      fill: type === 'line' ? false : true
    }))

    return { labels, datasets }
  }

  // Render charts with Chart.js
  return (
    <div className="data-visualization">
      <div className="chart-controls">
        <select value={chartType} onChange={(e) => setChartType(e.target.value)}>
          <option value="bar">Bar Chart</option>
          <option value="line">Line Chart</option>
          <option value="pie">Pie Chart</option>
          <option value="doughnut">Doughnut Chart</option>
        </select>
      </div>
      
      <div className="chart-container">
        {chartType === 'bar' && <Bar data={createChartData('bar')} />}
        {chartType === 'line' && <Line data={createChartData('line')} />}
        {chartType === 'pie' && <Pie data={createChartData('pie')} />}
        {chartType === 'doughnut' && <Doughnut data={createChartData('doughnut')} />}
      </div>
    </div>
  )
}
```

## Configuration & Environment

### Backend Configuration
```python
class Settings(BaseSettings):
    supabase_url: str
    supabase_anon_key: str
    supabase_service_key: str
    api_port: int = 8000
    api_host: str = "0.0.0.0"
    jwt_secret_key: str
    jwt_algorithm: str = "HS256"
    jwt_expiration_minutes: int = 60
    max_upload_size: int = 10 * 1024 * 1024  # 10MB
    allowed_extensions: str = ".xlsx"
    environment: str = "development"
    
    # OpenAI settings
    openai_api_key: str = ""
    openai_model: str = "gpt-4o"
    openai_temperature: float = 0.1
    openai_max_tokens: int = 1000
    
    # Email settings
    smtp_host: str = "smtp.gmail.com"
    smtp_port: int = 587
    smtp_username: str = ""
    smtp_password: str = ""
    
    # Redis settings
    redis_url: str = "redis://localhost:6379"
    
    class Config:
        env_file = ".env"
```

### Environment Template (CLIENT MUST CUSTOMIZE)
**CRITICAL SECURITY WARNING**: Client must generate all their own keys and credentials

```env
# CLIENT DATABASE CONFIGURATION (NO SHARED SERVICES)
# Client must set up their own isolated PostgreSQL database
DATABASE_URL=postgresql://CLIENT_USER:CLIENT_PASSWORD@CLIENT_HOST:5432/CLIENT_DATABASE
DATABASE_HOST=CLIENT_MANAGED_HOST_ONLY
DATABASE_USER=CLIENT_GENERATED_USER
DATABASE_PASSWORD=CLIENT_GENERATED_PASSWORD
DATABASE_NAME=CLIENT_SPECIFIC_DATABASE

# CLIENT-GENERATED SECURITY KEYS (NEVER SHARE OR REUSE)
JWT_SECRET_KEY=CLIENT_MUST_GENERATE_UNIQUE_SECRET
JWT_ALGORITHM=HS256
JWT_EXPIRATION_MINUTES=30

# CLIENT OPENAI CONFIGURATION (CLIENT'S OWN API KEY)
OPENAI_API_KEY=CLIENT_MUST_USE_THEIR_OWN_API_KEY
OPENAI_MODEL=gpt-4o
OPENAI_ORG_ID=CLIENT_OPENAI_ORG_ID

# CLIENT INFRASTRUCTURE CONFIGURATION
API_HOST=127.0.0.1  # Restrict to localhost initially
API_PORT=8000
ENVIRONMENT=CLIENT_ENVIRONMENT_NAME

# CLIENT-SPECIFIC IDENTIFIERS
CLIENT_ID=CLIENT_MUST_GENERATE_UNIQUE_UUID
CLIENT_NAME=CLIENT_ORGANIZATION_NAME
CLIENT_DEPLOYMENT_ID=CLIENT_GENERATED_DEPLOYMENT_ID

# SECURITY SETTINGS (CLIENT CONFIGURABLE)
MAX_UPLOAD_SIZE_MB=10
ALLOWED_FILE_EXTENSIONS=.xlsx
SESSION_TIMEOUT_MINUTES=30
MAX_LOGIN_ATTEMPTS=5
RATE_LIMIT_REQUESTS_PER_MINUTE=100

# OPTIONAL: CLIENT EMAIL CONFIGURATION (IF NEEDED)
SMTP_HOST=CLIENT_EMAIL_HOST
SMTP_PORT=587
SMTP_USERNAME=CLIENT_EMAIL_USER
SMTP_PASSWORD=CLIENT_EMAIL_PASSWORD
SMTP_FROM_EMAIL=CLIENT_EMAIL_ADDRESS

# OPTIONAL: CLIENT REDIS CONFIGURATION (IF USING BACKGROUND TASKS)
REDIS_URL=redis://CLIENT_REDIS_HOST:6379
```

## Key Business Logic

### Vendor-Specific Processing Rules

Implement these vendor detection and processing patterns:

1. **TaskifAI Format**:
   - Filename contains "Demo"
   - Sheets: "SalesPerSKU", "Info"
   - Columns: EANCode, SalesQuantity, SalesAmount
   - Extract month/year from filename

2. **Generic Format**:
   - Fallback for unknown formats
   - Auto-detect numeric and date columns
   - Apply standard cleaning rules

3. **Multi-vendor Support**:
   - Extensible detection system
   - Configurable column mappings
   - Vendor-specific transformations

### Data Processing Pipeline

1. **Upload**: Receive Excel file
2. **Validate**: Check file format and size
3. **Detect**: Identify vendor type
4. **Extract**: Read Excel data with appropriate parser
5. **Clean**: Apply vendor-specific cleaning rules
6. **Normalize**: Standardize data format
7. **Store**: Save to database with audit trail
8. **Notify**: Update processing status

### Security Requirements

1. **Authentication**: JWT-based with Supabase
2. **Authorization**: Row-Level Security (RLS)
3. **Data Isolation**: Users only see their own data
4. **Input Validation**: Sanitize all inputs
5. **SQL Injection Prevention**: Use parameterized queries
6. **File Upload Security**: Validate file types and sizes

## Success Criteria

The system should successfully:

1. âœ… Handle user registration and authentication
2. âœ… Process Excel file uploads with vendor detection
3. âœ… Clean and normalize data automatically
4. âœ… Display interactive charts and analytics
5. âœ… Respond to natural language queries about data
6. âœ… Generate and email reports
7. âœ… Maintain data security and user isolation
8. âœ… Scale to handle multiple concurrent users
9. âœ… Provide audit trails for all data transformations
10. âœ… Support adding new vendor formats easily

## Additional Features to Implement

1. **Real-time Updates**: WebSocket connections for live status
2. **Batch Processing**: Handle multiple file uploads
3. **Data Export**: Download processed data in various formats
4. **Advanced Analytics**: Time-series analysis, forecasting
5. **API Integration**: Webhook support for external systems
6. **Mobile Responsive**: Optimize for mobile devices
7. **Caching**: Redis-based caching for performance
8. **Monitoring**: Logging and error tracking
9. **Testing**: Comprehensive test suite
10. **Documentation**: API documentation with OpenAPI/Swagger

## CRITICAL SECURITY REQUIREMENTS

### Mandatory Security Implementation

**BEFORE DEPLOYMENT**: Client must implement ALL of the following security measures:

#### 1. Infrastructure Isolation
- âœ… **Separate Database**: Each client must use completely isolated PostgreSQL database
- âœ… **No Shared Services**: Zero shared infrastructure, keys, or user pools
- âœ… **Client-Owned Cloud**: All resources deployed in client's own cloud account
- âœ… **Network Isolation**: Private networks, no external database connections
- âœ… **Backup Isolation**: Client-managed backups with client-controlled encryption

#### 2. Authentication & Authorization 
- âœ… **Strong Password Policy**: Minimum 12 characters, complexity requirements
- âœ… **Multi-Factor Authentication**: MFA required for all users
- âœ… **Session Management**: Secure session handling with proper expiration
- âœ… **Rate Limiting**: Prevent brute force attacks on login endpoints
- âœ… **Client-Specific JWT**: Unique JWT secrets per client deployment

#### 3. Data Protection
- âœ… **Encryption at Rest**: All database data encrypted with client-managed keys
- âœ… **Encryption in Transit**: TLS 1.3 for all communications
- âœ… **Client ID Enforcement**: All data queries must include client_id filter
- âœ… **Input Sanitization**: All user inputs validated and sanitized
- âœ… **File Upload Security**: Strict file type and size validation

#### 4. Database Security
- âœ… **Row-Level Security**: RLS enabled on all tables with client_id enforcement  
- âœ… **SQL Injection Prevention**: Parameterized queries only, no dynamic SQL
- âœ… **Database User Isolation**: Separate database users per client
- âœ… **Connection Limits**: Proper connection pooling and limits
- âœ… **Audit Logging**: All database operations logged with client_id

#### 5. API Security
- âœ… **CORS Restrictions**: Strict CORS policy, no wildcard origins
- âœ… **Request Validation**: All API inputs validated with Pydantic
- âœ… **Rate Limiting**: Per-client rate limits on all endpoints
- âœ… **Error Handling**: No sensitive information in error responses
- âœ… **API Versioning**: Proper API versioning for security updates

#### 6. AI Security (OpenAI Integration)
- âœ… **Client API Keys**: Each client uses their own OpenAI API key
- âœ… **Query Sanitization**: AI queries filtered for dangerous SQL patterns
- âœ… **Result Filtering**: AI can only access client's own data
- âœ… **Usage Monitoring**: Track AI API usage per client
- âœ… **Prompt Injection Protection**: Validate AI prompts for malicious content

#### 7. File Processing Security
- âœ… **File Type Validation**: Only allow .xlsx files, validate file headers
- âœ… **Size Limits**: Enforce maximum file size limits per client
- âœ… **Malware Scanning**: Scan uploaded files for malicious content
- âœ… **Temporary Storage**: Secure temporary file handling with cleanup
- âœ… **Processing Isolation**: Each file processed in isolated environment

#### 8. Monitoring & Auditing
- âœ… **Security Event Logging**: Log all authentication, authorization, and data access events
- âœ… **Failed Login Monitoring**: Alert on suspicious login patterns
- âœ… **Data Access Auditing**: Track all data queries with user and timestamp
- âœ… **System Health Monitoring**: Monitor system performance and availability
- âœ… **Security Incident Response**: Plan for handling security incidents

### Security Testing Requirements

Before production deployment, client must perform:

1. **Penetration Testing**: Third-party security assessment
2. **Vulnerability Scanning**: Regular automated security scans
3. **Code Review**: Security-focused code review by security experts
4. **Database Security Audit**: Review all database permissions and policies
5. **Infrastructure Security**: Review cloud security configurations
6. **Compliance Verification**: Ensure meets relevant compliance requirements

### Production Security Checklist

- [ ] All default passwords changed
- [ ] All client-specific secrets generated
- [ ] Database isolation verified and tested
- [ ] API authentication working correctly
- [ ] File upload security tested
- [ ] AI query filtering verified
- [ ] Error handling sanitized
- [ ] Logging and monitoring configured
- [ ] Backup and recovery procedures tested
- [ ] Security incident response plan documented
- [ ] Staff security training completed

### Security Violation Consequences

**CRITICAL**: Any security breach due to:
- Shared databases between clients
- Hardcoded credentials or secrets
- Missing client_id filtering
- Cross-client data access
- Insufficient input validation

Will result in immediate system compromise and potential data loss.

---

## Final Implementation Notes

**This specification provides complete instructions for building a secure, client-isolated data processing platform. Each client deployment must be completely independent with no shared resources, ensuring maximum security and data isolation.**

**Security is not optional - it is the foundation of this entire system.**