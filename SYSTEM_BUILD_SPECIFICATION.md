# Secure Data Processing Platform - Complete System Build Specification

## System Overview

Build an AI-powered reseller data management platform that transforms messy Excel spreadsheets into clean, actionable insights through automatic data processing, real-time dashboards, and AI-powered natural language analytics.

**SECURITY FIRST**: This system is designed for complete data isolation. Each client deployment must use entirely separate infrastructure with no shared resources.

## Core Architecture

### Technology Stack
- **Backend**: FastAPI (Python 3.12+)
- **Frontend**: React 18+ with Vite
- **Database**: PostgreSQL with Row-Level Security (client-managed)
- **AI Integration**: OpenAI GPT-4 with LangChain (client API keys)
- **Charts**: Chart.js with react-chartjs-2
- **Authentication**: JWT (client-generated secrets)
- **File Processing**: pandas + openpyxl
- **Background Tasks**: Celery with Redis (optional, client-managed)

### Security Principles
1. **Complete Data Isolation**: No shared databases, keys, or infrastructure
2. **Client-Owned Infrastructure**: Each deployment uses client's own cloud resources
3. **Zero Trust Architecture**: No pre-configured connections or defaults
4. **Client-Generated Secrets**: All encryption keys generated by client
5. **No Cross-Client Dependencies**: System cannot access external databases

### System Components
1. **File Upload & Processing Pipeline**
2. **Vendor Detection & Data Normalization**
3. **Real-time Dashboard with Analytics**
4. **AI-Powered Chat Interface**
5. **Email Reporting System**
6. **User Authentication & Security**

## Database Schema Playbook

### Database Setup Requirements

**CRITICAL**: Each client must set up their own isolated PostgreSQL database with the following characteristics:
- Dedicated database instance (no shared hosting)
- Client-generated database credentials
- Client-controlled access policies
- No external database connections allowed

### Core Tables Structure

```sql
-- Enable UUID extension
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- Users table (for client's authentication system)
CREATE TABLE users (
  id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
  email text UNIQUE NOT NULL,
  password_hash text NOT NULL, -- Client must implement secure hashing
  created_at timestamptz DEFAULT now(),
  updated_at timestamptz DEFAULT now(),
  is_active boolean DEFAULT true
);

-- Products catalog (client-specific products only)
CREATE TABLE products (
  ean text PRIMARY KEY,
  name text,
  brand text, -- Client configures their own brand
  created_at timestamptz DEFAULT now(),
  client_id uuid NOT NULL -- Ensures data belongs to this client only
);

-- Upload tracking
CREATE TABLE public.uploads (
  id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id uuid REFERENCES public.users(id) ON DELETE CASCADE,
  filename text NOT NULL,
  file_size bigint,
  uploaded_at timestamptz DEFAULT now(),
  status text CHECK (status IN ('pending', 'processing', 'completed', 'failed')),
  error_message text,
  rows_processed integer,
  rows_cleaned integer,
  processing_time_ms integer
);

-- Main sales data table (isolated per client)
CREATE TABLE sales_data (
  id uuid NOT NULL DEFAULT gen_random_uuid(),
  upload_id uuid REFERENCES uploads(id) ON DELETE SET NULL,
  product_ean text REFERENCES products(ean),
  month integer CHECK (month >= 1 AND month <= 12),
  year integer CHECK (year >= 2000),
  quantity integer,
  sales_amount_local text, -- Local currency amount
  sales_amount_base numeric, -- Base currency for client
  currency text,
  created_at timestamp DEFAULT now(),
  reseller text,
  functional_name text,
  client_id uuid NOT NULL, -- Mandatory client isolation
  CONSTRAINT sales_data_pkey PRIMARY KEY (id)
);

-- Processed/cleaned data storage (client-isolated)
CREATE TABLE processed_data (
  id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
  upload_id uuid REFERENCES uploads(id) ON DELETE CASCADE,
  product_ean text,
  month integer,
  year integer,
  quantity integer,
  sales_amount_local text,
  sales_amount_base numeric,
  currency text,
  reseller text,
  functional_name text,
  sales_date date,
  created_at timestamptz DEFAULT now(),
  client_id uuid NOT NULL -- Ensures no cross-client data access
);

-- Data transformation audit trail
CREATE TABLE public.transform_logs (
  id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
  upload_id uuid REFERENCES public.uploads(id) ON DELETE CASCADE,
  row_index integer,
  column_name text,
  original_value text,
  cleaned_value text,
  transformation_type text,
  logged_at timestamptz DEFAULT now()
);

-- Step-by-step processing logs
CREATE TABLE public.processing_logs (
  id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
  upload_id uuid REFERENCES public.uploads(id) ON DELETE CASCADE,
  step_name text NOT NULL,
  step_status text CHECK (step_status IN ('started', 'completed', 'failed')),
  message text,
  details jsonb,
  created_at timestamptz DEFAULT now()
);
```

### Row-Level Security (RLS)

```sql
-- Enable RLS on all tables
ALTER TABLE public.users ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.uploads ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.sellout_entries2 ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.mock_data ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.transform_logs ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.processing_logs ENABLE ROW LEVEL SECURITY;
ALTER TABLE public.products ENABLE ROW LEVEL SECURITY;

-- User policies
CREATE POLICY "Users can view own profile" ON public.users
  FOR SELECT USING (auth.uid() = id);

CREATE POLICY "Users can view own uploads" ON public.uploads
  FOR SELECT USING (auth.uid() = user_id);

CREATE POLICY "Users can create own uploads" ON public.uploads
  FOR INSERT WITH CHECK (auth.uid() = user_id);

-- Data access policies (users only see their own data)
CREATE POLICY "Users can view own sellout entries" ON public.sellout_entries2
  FOR SELECT USING (
    EXISTS (
      SELECT 1 FROM public.uploads
      WHERE uploads.id = sellout_entries2.upload_id
      AND uploads.user_id = auth.uid()
    )
  );

-- Service role policies for background processing
CREATE POLICY "Service role can manage all data" ON public.uploads
  FOR ALL USING (auth.role() = 'service_role');

-- Products are readable by all
CREATE POLICY "Anyone can view products" ON public.products
  FOR SELECT USING (true);
```

### SQL Function for Chat Queries

```sql
CREATE OR REPLACE FUNCTION execute_sql_query(query_text text)
RETURNS jsonb
LANGUAGE plpgsql
SECURITY DEFINER
AS $$
DECLARE
  result jsonb;
  rec record;
  results jsonb[] := '{}';
BEGIN
  -- Security: Only allow SELECT statements
  IF NOT (upper(trim(query_text)) LIKE 'SELECT%') THEN
    RAISE EXCEPTION 'Only SELECT statements are allowed';
  END IF;
  
  -- Security: Block dangerous keywords
  IF upper(query_text) ~ '(DROP|DELETE|UPDATE|INSERT|ALTER|CREATE|GRANT|REVOKE|EXEC|EXECUTE)' THEN
    RAISE EXCEPTION 'Dangerous SQL keywords are not allowed';
  END IF;
  
  FOR rec IN EXECUTE safe_query LOOP
    results := results || to_jsonb(rec);
  END LOOP;
  
  -- Return results as JSON
  RETURN array_to_json(results)::jsonb;
  
EXCEPTION
  WHEN OTHERS THEN
    -- Log security violation attempts
    INSERT INTO security_logs (client_id, attempted_query, error_message, created_at)
    VALUES (client_id_param, query_text, SQLERRM, now());
    
    -- Return sanitized error (don't expose internal structure)
    RETURN jsonb_build_object(
      'error', true,
      'message', 'Query execution failed',
      'timestamp', now()
    );
END;
$$;

-- Security audit log table (CLIENT MUST IMPLEMENT)
CREATE TABLE security_logs (
  id uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
  client_id uuid NOT NULL,
  attempted_query text,
  error_message text,
  created_at timestamptz DEFAULT now(),
  ip_address inet,
  user_agent text
);
```

## Backend Implementation

### Project Structure
```
backend/
├── app/
│   ├── __init__.py
│   ├── api/
│   │   ├── auth.py          # JWT authentication
│   │   ├── upload.py        # File upload endpoints
│   │   ├── status.py        # Processing status
│   │   ├── dashboard.py     # Analytics endpoints
│   │   ├── chat.py          # AI chat interface
│   │   ├── email.py         # Email reporting
│   │   └── webhook.py       # External integrations
│   ├── models/
│   │   ├── auth.py          # User models
│   │   └── upload.py        # Upload models
│   ├── pipeline/
│   │   ├── cleaners.py      # Data cleaning logic
│   │   ├── detector.py      # Vendor detection
│   │   └── normalizers.py   # Data normalization
│   ├── services/
│   │   ├── auth_service.py     # Authentication logic
│   │   ├── cleaning_service.py # File processing
│   │   ├── db_service.py       # Database operations
│   │   ├── email_service.py    # Email functionality
│   │   ├── file_service.py     # File handling
│   │   └── report_service.py   # Report generation
│   └── utils/
│       ├── config.py        # Configuration
│       └── exceptions.py    # Custom exceptions
├── main.py                  # FastAPI app entry point
├── requirements.txt         # Dependencies
└── run.sh                  # Startup script
```

### Dependencies (requirements.txt)
```
fastapi==0.109.0
uvicorn[standard]==0.27.0
python-multipart==0.0.6
pandas==2.1.4
openpyxl==3.1.2
supabase==2.9.0
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-dotenv==1.0.0
pydantic==2.5.3
pydantic-settings==2.1.0
email-validator==2.1.0
jinja2==3.1.2
reportlab==4.0.7
celery[redis]==5.3.4
requests==2.31.0
langchain==0.2.16
langchain-openai==0.1.25
langchain-community==0.2.17
psycopg2-binary==2.9.9
sqlalchemy==2.0.30
```

### Core API Endpoints

#### Authentication API (`/api/auth`)
```python
@router.post("/register")
async def register(user_data: UserRegistration)

@router.post("/login") 
async def login(credentials: UserLogin)

@router.get("/debug-token")
async def debug_token(current_user = Depends(get_current_user))

@router.post("/logout")
async def logout()
```

#### Upload API (`/api/upload`)
```python
@router.post("/")
async def upload_file(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    current_user: dict = Depends(get_current_user)
)

@router.post("/multiple")
async def upload_multiple_files(
    files: List[UploadFile],
    current_user: dict = Depends(get_current_user)
)
```

#### Status API (`/api/status`)
```python
@router.get("/")
async def get_upload_status(current_user = Depends(get_current_user))

@router.get("/{upload_id}")
async def get_specific_upload_status(upload_id: str)

@router.get("/{upload_id}/logs")
async def get_processing_logs(upload_id: str)
```

#### Dashboard API (`/api/dashboards`)
```python
@router.get("/analytics")
async def get_analytics_data(current_user = Depends(get_current_user))

@router.get("/sales-summary")
async def get_sales_summary()

@router.get("/monthly-trends")
async def get_monthly_trends()

@router.get("/top-products")
async def get_top_products()
```

#### Chat API (`/api/chat`)
```python
@router.post("/query")
async def chat_query(request: ChatRequest)

@router.get("/suggestions")
async def get_query_suggestions()
```

### Vendor Detection System

Implement intelligent vendor detection based on filename patterns and Excel content:

```python
class VendorDetector:
    def detect_vendor(self, filename: str, df: pd.DataFrame) -> str:
        """Detect vendor based on filename and sheet content"""
        
        filename_lower = filename.lower()
        
        # Filename pattern detection
        patterns = {
            "galilu": ["galilu"],
            "boxnox": ["boxnox"],
            "skins_sa": ["skins sa"],
            "skins_nl": ["bibbiparfu"],
            "cdlc": ["cdlc", "bibbi.*sell_out"],
            "liberty": ["continuity"],
            "taskifai": ["demo", "taskif"]  # Add this for the demo
        }
        
        for vendor, pattern_list in patterns.items():
            if any(re.search(pattern, filename_lower) for pattern in pattern_list):
                return vendor
        
        # Sheet name detection
        if hasattr(df, 'sheet_names'):
            sheet_names = [s.lower() for s in df.sheet_names]
            if any("salespersku" in s for s in sheet_names):
                return "skins_nl"
            elif any("sell out by ean" in s for s in sheet_names):
                return "boxnox"
        
        return "unknown"
```

### Data Cleaning Pipeline

```python
class DataCleaner:
    async def clean_data(self, df: pd.DataFrame, vendor: str, filename: str) -> Tuple[pd.DataFrame, List[Dict]]:
        """Clean data based on vendor-specific rules"""
        
        df_clean = df.copy()
        transformations = []
        
        # Remove empty rows
        df_clean = df_clean.dropna(how='all')
        
        # Apply vendor-specific cleaning
        if vendor == "taskifai":
            df_clean = await self._clean_taskifai_data(df_clean, filename)
        elif vendor == "galilu":
            df_clean = await self._clean_galilu_data(df_clean)
        # ... other vendors
        
        # Standardize numeric values
        df_clean = self._standardize_numeric_columns(df_clean)
        
        return df_clean, transformations
    
    def _clean_numeric_value(self, value):
        """Standardized numeric cleaning for sales values"""
        if pd.isna(value) or value == '':
            return None
        
        clean_value = str(value).strip()
        
        # Replace various dash types with ASCII hyphen
        clean_value = re.sub(r'[\u2013\u2014\u2212]', '-', clean_value)
        
        # Replace comma decimal separator with dot
        clean_value = clean_value.replace(',', '.')
        
        # Remove currency symbols
        clean_value = clean_value.replace('$', '').replace('£', '').replace('€', '').replace(' ', '')
        
        try:
            return float(clean_value)
        except (ValueError, TypeError):
            return None
```

### AI Chat Integration

```python
class ChatService:
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4o",
            temperature=0.1,
            max_tokens=1000
        )
        
    async def process_query(self, message: str, user_id: str) -> dict:
        """Process natural language query about user's data"""
        
        # Create custom SQL database interface
        db = SupabaseSQLDatabase(user_id)
        
        # Create SQL agent
        toolkit = SQLDatabaseToolkit(db=db, llm=self.llm)
        agent = create_sql_agent(
            llm=self.llm,
            toolkit=toolkit,
            agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True
        )
        
        try:
            response = agent.run(message)
            return {
                "response": response,
                "sql_query": self._extract_sql_from_response(response),
                "success": True
            }
        except Exception as e:
            return {
                "response": f"I couldn't process that query: {str(e)}",
                "success": False
            }
```

## Frontend Implementation

### Project Structure
```
frontend/
├── src/
│   ├── App.jsx              # Main app component
│   ├── main.jsx            # React entry point
│   ├── components/
│   │   ├── AnalyticsDashboard.jsx  # Main dashboard
│   │   ├── Upload.jsx              # File upload
│   │   ├── DataVisualization.jsx   # Charts
│   │   ├── Chat.jsx               # AI chat interface
│   │   ├── ChatSection.jsx        # Chat component
│   │   ├── StatusList.jsx         # Upload status
│   │   ├── EmailReporting.jsx     # Email reports
│   │   └── LandingPage.jsx        # Welcome page
│   ├── pages/
│   │   ├── Dashboard.jsx     # Dashboard page
│   │   └── Login.jsx        # Authentication
│   ├── services/
│   │   ├── api.js          # API client
│   │   └── supabase.js     # Supabase client
│   ├── styles/
│   │   └── index.css       # Global styles
│   └── utils/
│       └── mathRenderer.jsx  # Math equation rendering
├── package.json
├── vite.config.js
└── index.html
```

### Dependencies (package.json)
```json
{
  "name": "data-cleaning-frontend",
  "version": "1.0.0",
  "type": "module",
  "dependencies": {
    "@supabase/supabase-js": "^2.39.3",
    "chart.js": "^4.5.0",
    "katex": "^0.16.22",
    "react": "^18.2.0",
    "react-chartjs-2": "^5.3.0",
    "react-dom": "^18.2.0",
    "react-katex": "^3.1.0"
  },
  "devDependencies": {
    "@types/react": "^18.2.47",
    "@types/react-dom": "^18.2.18",
    "@vitejs/plugin-react": "^4.2.1",
    "vite": "^7.0.5"
  },
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview"
  }
}
```

### Main App Component
```jsx
function App() {
  const [isAuthenticated, setIsAuthenticated] = useState(false)
  const [loading, setLoading] = useState(true)
  const [user, setUser] = useState(null)

  useEffect(() => {
    checkAuthStatus()
  }, [])

  const checkAuthStatus = async () => {
    try {
      const token = localStorage.getItem('access_token')
      if (!token) {
        setLoading(false)
        return
      }

      const response = await fetch('http://localhost:8000/api/auth/debug-token', {
        headers: { 'Authorization': `Bearer ${token}` }
      })

      if (!response.ok) throw new Error('Token validation failed')

      const debugInfo = await response.json()
      if (debugInfo.user_found) {
        setIsAuthenticated(true)
        setUser({
          email: debugInfo.user_email,
          id: debugInfo.user_id
        })
      } else {
        localStorage.removeItem('access_token')
        setIsAuthenticated(false)
      }
    } catch (error) {
      localStorage.removeItem('access_token')
      setIsAuthenticated(false)
    } finally {
      setLoading(false)
    }
  }

  if (loading) return <div className="loading">Loading...</div>

  return (
    <div className="app">
      {!isAuthenticated ? (
        <Login onLoginSuccess={checkAuthStatus} />
      ) : (
        <Dashboard user={user} onLogout={() => {
          localStorage.removeItem('access_token')
          setIsAuthenticated(false)
          setUser(null)
        }} />
      )}
    </div>
  )
}
```

### Dashboard Component
```jsx
const Dashboard = ({ user, onLogout }) => {
  const [activeTab, setActiveTab] = useState('upload')
  const [uploads, setUploads] = useState([])
  const [analytics, setAnalytics] = useState(null)

  // Tab navigation
  const tabs = [
    { id: 'upload', name: 'Upload', icon: '📤' },
    { id: 'analytics', name: 'Analytics', icon: '📊' },
    { id: 'chat', name: 'AI Chat', icon: '🤖' },
    { id: 'email', name: 'Reports', icon: '📧' }
  ]

  return (
    <div className="dashboard">
      <header className="dashboard-header">
        <h1>TaskifAI Dashboard</h1>
        <div className="user-info">
          <span>{user?.email}</span>
          <button onClick={onLogout}>Logout</button>
        </div>
      </header>
      
      <nav className="dashboard-nav">
        {tabs.map(tab => (
          <button
            key={tab.id}
            className={`nav-tab ${activeTab === tab.id ? 'active' : ''}`}
            onClick={() => setActiveTab(tab.id)}
          >
            {tab.icon} {tab.name}
          </button>
        ))}
      </nav>
      
      <main className="dashboard-content">
        {activeTab === 'upload' && <Upload />}
        {activeTab === 'analytics' && <AnalyticsDashboard />}
        {activeTab === 'chat' && <ChatSection />}
        {activeTab === 'email' && <EmailReporting />}
      </main>
    </div>
  )
}
```

### Data Visualization Component
```jsx
const DataVisualization = ({ results, sqlQuery, originalMessage }) => {
  const [viewMode, setViewMode] = useState('auto')
  const [chartType, setChartType] = useState('bar')
  
  const analyzeDataForVisualization = () => {
    if (!results || results.length === 0) return null
    
    const firstRow = results[0]
    const columns = Object.keys(firstRow)
    const numericColumns = columns.filter(col => {
      return results.some(row => 
        row[col] !== null && 
        !isNaN(Number(row[col])) &&
        Number(row[col]) !== 0
      )
    })
    
    const textColumns = columns.filter(col => 
      !numericColumns.includes(col) && 
      typeof firstRow[col] === 'string'
    )
    
    return {
      columns,
      numericColumns,
      textColumns,
      hasNumericData: numericColumns.length > 0,
      hasCategories: textColumns.length > 0,
      suggestedChart: getSuggestedChartType(results, numericColumns, textColumns)
    }
  }

  const createChartData = (type) => {
    const analysis = analyzeDataForVisualization()
    if (!analysis?.hasNumericData || !analysis?.hasCategories) return null

    const labelColumn = analysis.textColumns[0]
    const valueColumns = analysis.numericColumns.slice(0, 3)
    
    const labels = results.map(row => String(row[labelColumn] || 'Unknown'))

    if (type === 'pie' || type === 'doughnut') {
      const values = results.map(row => Number(row[valueColumns[0]]) || 0)
      return {
        labels,
        datasets: [{
          data: values,
          backgroundColor: [
            '#FF6384', '#36A2EB', '#FFCE56', '#4BC0C0',
            '#9966FF', '#FF9F40', '#FF6384', '#C9CBCF'
          ],
          label: valueColumns[0]
        }]
      }
    }

    // Bar/Line charts with multiple series
    const datasets = valueColumns.map((col, index) => ({
      label: col,
      data: results.map(row => Number(row[col]) || 0),
      backgroundColor: `hsl(${index * 60}, 70%, 50%)`,
      borderColor: `hsl(${index * 60}, 70%, 40%)`,
      borderWidth: 2,
      fill: type === 'line' ? false : true
    }))

    return { labels, datasets }
  }

  // Render charts with Chart.js
  return (
    <div className="data-visualization">
      <div className="chart-controls">
        <select value={chartType} onChange={(e) => setChartType(e.target.value)}>
          <option value="bar">Bar Chart</option>
          <option value="line">Line Chart</option>
          <option value="pie">Pie Chart</option>
          <option value="doughnut">Doughnut Chart</option>
        </select>
      </div>
      
      <div className="chart-container">
        {chartType === 'bar' && <Bar data={createChartData('bar')} />}
        {chartType === 'line' && <Line data={createChartData('line')} />}
        {chartType === 'pie' && <Pie data={createChartData('pie')} />}
        {chartType === 'doughnut' && <Doughnut data={createChartData('doughnut')} />}
      </div>
    </div>
  )
}
```

## Configuration & Environment

### Backend Configuration
```python
class Settings(BaseSettings):
    supabase_url: str
    supabase_anon_key: str
    supabase_service_key: str
    api_port: int = 8000
    api_host: str = "0.0.0.0"
    jwt_secret_key: str
    jwt_algorithm: str = "HS256"
    jwt_expiration_minutes: int = 60
    max_upload_size: int = 10 * 1024 * 1024  # 10MB
    allowed_extensions: str = ".xlsx"
    environment: str = "development"
    
    # OpenAI settings
    openai_api_key: str = ""
    openai_model: str = "gpt-4o"
    openai_temperature: float = 0.1
    openai_max_tokens: int = 1000
    
    # Email settings
    smtp_host: str = "smtp.gmail.com"
    smtp_port: int = 587
    smtp_username: str = ""
    smtp_password: str = ""
    
    # Redis settings
    redis_url: str = "redis://localhost:6379"
    
    class Config:
        env_file = ".env"
```

### Environment Template (CLIENT MUST CUSTOMIZE)
**CRITICAL SECURITY WARNING**: Client must generate all their own keys and credentials

```env
# CLIENT DATABASE CONFIGURATION (NO SHARED SERVICES)
# Client must set up their own isolated PostgreSQL database
DATABASE_URL=postgresql://CLIENT_USER:CLIENT_PASSWORD@CLIENT_HOST:5432/CLIENT_DATABASE
DATABASE_HOST=CLIENT_MANAGED_HOST_ONLY
DATABASE_USER=CLIENT_GENERATED_USER
DATABASE_PASSWORD=CLIENT_GENERATED_PASSWORD
DATABASE_NAME=CLIENT_SPECIFIC_DATABASE

# CLIENT-GENERATED SECURITY KEYS (NEVER SHARE OR REUSE)
JWT_SECRET_KEY=CLIENT_MUST_GENERATE_UNIQUE_SECRET
JWT_ALGORITHM=HS256
JWT_EXPIRATION_MINUTES=30

# CLIENT OPENAI CONFIGURATION (CLIENT'S OWN API KEY)
OPENAI_API_KEY=CLIENT_MUST_USE_THEIR_OWN_API_KEY
OPENAI_MODEL=gpt-4o
OPENAI_ORG_ID=CLIENT_OPENAI_ORG_ID

# CLIENT INFRASTRUCTURE CONFIGURATION
API_HOST=127.0.0.1  # Restrict to localhost initially
API_PORT=8000
ENVIRONMENT=CLIENT_ENVIRONMENT_NAME

# CLIENT-SPECIFIC IDENTIFIERS
CLIENT_ID=CLIENT_MUST_GENERATE_UNIQUE_UUID
CLIENT_NAME=CLIENT_ORGANIZATION_NAME
CLIENT_DEPLOYMENT_ID=CLIENT_GENERATED_DEPLOYMENT_ID

# SECURITY SETTINGS (CLIENT CONFIGURABLE)
MAX_UPLOAD_SIZE_MB=10
ALLOWED_FILE_EXTENSIONS=.xlsx
SESSION_TIMEOUT_MINUTES=30
MAX_LOGIN_ATTEMPTS=5
RATE_LIMIT_REQUESTS_PER_MINUTE=100

# OPTIONAL: CLIENT EMAIL CONFIGURATION (IF NEEDED)
SMTP_HOST=CLIENT_EMAIL_HOST
SMTP_PORT=587
SMTP_USERNAME=CLIENT_EMAIL_USER
SMTP_PASSWORD=CLIENT_EMAIL_PASSWORD
SMTP_FROM_EMAIL=CLIENT_EMAIL_ADDRESS

# OPTIONAL: CLIENT REDIS CONFIGURATION (IF USING BACKGROUND TASKS)
REDIS_URL=redis://CLIENT_REDIS_HOST:6379
```

## Key Business Logic

### Vendor-Specific Processing Rules

Implement these vendor detection and processing patterns:

1. **TaskifAI Format**:
   - Filename contains "Demo"
   - Sheets: "SalesPerSKU", "Info"
   - Columns: EANCode, SalesQuantity, SalesAmount
   - Extract month/year from filename

2. **Generic Format**:
   - Fallback for unknown formats
   - Auto-detect numeric and date columns
   - Apply standard cleaning rules

3. **Multi-vendor Support**:
   - Extensible detection system
   - Configurable column mappings
   - Vendor-specific transformations

### Data Processing Pipeline

1. **Upload**: Receive Excel file
2. **Validate**: Check file format and size
3. **Detect**: Identify vendor type
4. **Extract**: Read Excel data with appropriate parser
5. **Clean**: Apply vendor-specific cleaning rules
6. **Normalize**: Standardize data format
7. **Store**: Save to database with audit trail
8. **Notify**: Update processing status

### Security Requirements

1. **Authentication**: JWT-based with Supabase
2. **Authorization**: Row-Level Security (RLS)
3. **Data Isolation**: Users only see their own data
4. **Input Validation**: Sanitize all inputs
5. **SQL Injection Prevention**: Use parameterized queries
6. **File Upload Security**: Validate file types and sizes

## Success Criteria

The system should successfully:

1. ✅ Handle user registration and authentication
2. ✅ Process Excel file uploads with vendor detection
3. ✅ Clean and normalize data automatically
4. ✅ Display interactive charts and analytics
5. ✅ Respond to natural language queries about data
6. ✅ Generate and email reports
7. ✅ Maintain data security and user isolation
8. ✅ Scale to handle multiple concurrent users
9. ✅ Provide audit trails for all data transformations
10. ✅ Support adding new vendor formats easily

## Additional Features to Implement

1. **Real-time Updates**: WebSocket connections for live status
2. **Batch Processing**: Handle multiple file uploads
3. **Data Export**: Download processed data in various formats
4. **Advanced Analytics**: Time-series analysis, forecasting
5. **API Integration**: Webhook support for external systems
6. **Mobile Responsive**: Optimize for mobile devices
7. **Caching**: Redis-based caching for performance
8. **Monitoring**: Logging and error tracking
9. **Testing**: Comprehensive test suite
10. **Documentation**: API documentation with OpenAPI/Swagger

## CRITICAL SECURITY REQUIREMENTS

### Mandatory Security Implementation

**BEFORE DEPLOYMENT**: Client must implement ALL of the following security measures:

#### 1. Infrastructure Isolation
- ✅ **Separate Database**: Each client must use completely isolated PostgreSQL database
- ✅ **No Shared Services**: Zero shared infrastructure, keys, or user pools
- ✅ **Client-Owned Cloud**: All resources deployed in client's own cloud account
- ✅ **Network Isolation**: Private networks, no external database connections
- ✅ **Backup Isolation**: Client-managed backups with client-controlled encryption

#### 2. Authentication & Authorization 
- ✅ **Strong Password Policy**: Minimum 12 characters, complexity requirements
- ✅ **Multi-Factor Authentication**: MFA required for all users
- ✅ **Session Management**: Secure session handling with proper expiration
- ✅ **Rate Limiting**: Prevent brute force attacks on login endpoints
- ✅ **Client-Specific JWT**: Unique JWT secrets per client deployment

#### 3. Data Protection
- ✅ **Encryption at Rest**: All database data encrypted with client-managed keys
- ✅ **Encryption in Transit**: TLS 1.3 for all communications
- ✅ **Client ID Enforcement**: All data queries must include client_id filter
- ✅ **Input Sanitization**: All user inputs validated and sanitized
- ✅ **File Upload Security**: Strict file type and size validation

#### 4. Database Security
- ✅ **Row-Level Security**: RLS enabled on all tables with client_id enforcement  
- ✅ **SQL Injection Prevention**: Parameterized queries only, no dynamic SQL
- ✅ **Database User Isolation**: Separate database users per client
- ✅ **Connection Limits**: Proper connection pooling and limits
- ✅ **Audit Logging**: All database operations logged with client_id

#### 5. API Security
- ✅ **CORS Restrictions**: Strict CORS policy, no wildcard origins
- ✅ **Request Validation**: All API inputs validated with Pydantic
- ✅ **Rate Limiting**: Per-client rate limits on all endpoints
- ✅ **Error Handling**: No sensitive information in error responses
- ✅ **API Versioning**: Proper API versioning for security updates

#### 6. AI Security (OpenAI Integration)
- ✅ **Client API Keys**: Each client uses their own OpenAI API key
- ✅ **Query Sanitization**: AI queries filtered for dangerous SQL patterns
- ✅ **Result Filtering**: AI can only access client's own data
- ✅ **Usage Monitoring**: Track AI API usage per client
- ✅ **Prompt Injection Protection**: Validate AI prompts for malicious content

#### 7. File Processing Security
- ✅ **File Type Validation**: Only allow .xlsx files, validate file headers
- ✅ **Size Limits**: Enforce maximum file size limits per client
- ✅ **Malware Scanning**: Scan uploaded files for malicious content
- ✅ **Temporary Storage**: Secure temporary file handling with cleanup
- ✅ **Processing Isolation**: Each file processed in isolated environment

#### 8. Monitoring & Auditing
- ✅ **Security Event Logging**: Log all authentication, authorization, and data access events
- ✅ **Failed Login Monitoring**: Alert on suspicious login patterns
- ✅ **Data Access Auditing**: Track all data queries with user and timestamp
- ✅ **System Health Monitoring**: Monitor system performance and availability
- ✅ **Security Incident Response**: Plan for handling security incidents

### Security Testing Requirements

Before production deployment, client must perform:

1. **Penetration Testing**: Third-party security assessment
2. **Vulnerability Scanning**: Regular automated security scans
3. **Code Review**: Security-focused code review by security experts
4. **Database Security Audit**: Review all database permissions and policies
5. **Infrastructure Security**: Review cloud security configurations
6. **Compliance Verification**: Ensure meets relevant compliance requirements

### Production Security Checklist

- [ ] All default passwords changed
- [ ] All client-specific secrets generated
- [ ] Database isolation verified and tested
- [ ] API authentication working correctly
- [ ] File upload security tested
- [ ] AI query filtering verified
- [ ] Error handling sanitized
- [ ] Logging and monitoring configured
- [ ] Backup and recovery procedures tested
- [ ] Security incident response plan documented
- [ ] Staff security training completed

### Security Violation Consequences

**CRITICAL**: Any security breach due to:
- Shared databases between clients
- Hardcoded credentials or secrets
- Missing client_id filtering
- Cross-client data access
- Insufficient input validation

Will result in immediate system compromise and potential data loss.

---

## Final Implementation Notes

**This specification provides complete instructions for building a secure, client-isolated data processing platform. Each client deployment must be completely independent with no shared resources, ensuring maximum security and data isolation.**

**Security is not optional - it is the foundation of this entire system.**